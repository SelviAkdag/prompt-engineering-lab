{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "135a8503",
   "metadata": {},
   "source": [
    "# Prompt Engineering Lab\n",
    "\n",
    "**From Zero-shot to Self-Consistency â€” A Practical Introduction to Prompt Engineering** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7631ec1c",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to this interactive notebook on **Text Classification with Large Language Models (LLMs)**.  \n",
    "In this lesson, you will explore how LLMs can be guided using **different prompting techniques** â€“ such as *zero-shot*, *few-shot*, and *chain-of-thought (CoT)* â€“ to perform classification tasks.\n",
    "\n",
    "### LLM Backend\n",
    "\n",
    "This notebook uses **Ollama** (local model) or **OpenAI API** for inference.  \n",
    "Make sure you've completed the setup in `index.html` before proceeding.\n",
    "\n",
    "**Current configuration:**\n",
    "- Backend: `ollama` (or `openai` if configured)\n",
    "- Model: `llama3.2-long` (Ollama) or `gpt-4o-mini` (OpenAI)\n",
    "\n",
    "If you encounter errors, verify that:\n",
    "- Ollama is running: `ollama list` should show `llama3.2-long`\n",
    "- `.env` file is configured correctly\n",
    "\n",
    "#### Throughout this notebook, you will:\n",
    "- Understand the **core concepts** behind each prompting method.\n",
    "- Experiment with **real examples**.\n",
    "- Test your understanding through **interactive quizzes**.\n",
    "- Receive hints and feedback along the way.\n",
    "\n",
    "#### Note about the code cells:\n",
    "\n",
    "This notebook follows a clean architecture pattern where **all implementation details are kept in separate modules** under the `src/` folder. \n",
    "\n",
    "**If you're curious about how things work under the hood:**\n",
    "- **Prompting logic** (how prompts are built): `src/prompting.py`\n",
    "- **LLM calls** (API communication): `src/llm.py`\n",
    "- **Evaluation metrics** (accuracy, precision, recall, F1): `src/metrics.py`\n",
    "- **Interactive widgets & visualizations**: `src/notebook_helpers.py`\n",
    "- **Progress tracking**: `src/progress.py`\n",
    "- **Quiz answers & feedback**: `src/quiz_answers.py`\n",
    "\n",
    "This design keeps the notebook **focused on learning**, not implementation details. You can always explore the source files if you want to understand or modify the underlying code! ğŸ”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3832e6a0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Setup & imports\n",
    "\n",
    "Let's begin with the first and simplest approach â€“ **Zero-shot prompting**. But first let's run this very first cell to complete the setup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7c63be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === IMPORTS (run this cell first) ===\n",
    "import sys, os, csv, pathlib, subprocess\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "import time\n",
    "\n",
    "# Project structure\n",
    "project_root = Path.cwd().parent\n",
    "os.chdir(project_root)\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# My own modules\n",
    "from src.config import MODEL_BACKEND, OLLAMA_MODEL, OPENAI_MODEL\n",
    "from src.llm import call_llm\n",
    "from src.progress import load_progress, save_progress\n",
    "from src.metrics import precision_recall_f1\n",
    "from src.quiz_answers import check_answer\n",
    "from src.prompting import (\n",
    "    run_zero_shot, \n",
    "    run_few_shot, \n",
    "    run_cot, \n",
    "    run_self_consistency,\n",
    "    build_zero_shot_prompt, \n",
    "    build_few_shot_prompt, \n",
    "    build_cot_prompt, \n",
    "    normalize_label\n",
    ")\n",
    "from src.notebook_helpers import (\n",
    "    # Zero-shot\n",
    "    display_zero_shot_example,\n",
    "    create_zero_shot_interactive,\n",
    "    create_zero_shot_quiz,\n",
    "    run_zero_shot_full_dataset,\n",
    "    # Few-shot\n",
    "    display_few_shot_example,\n",
    "    create_few_shot_interactive,\n",
    "    create_few_shot_quiz,\n",
    "    run_few_shot_full_dataset,\n",
    "    # CoT\n",
    "    display_cot_example,\n",
    "    create_cot_interactive,\n",
    "    create_cot_quiz,\n",
    "    run_cot_full_dataset,\n",
    "    # Self-Consistency\n",
    "    display_self_consistency_example,\n",
    "    create_self_consistency_interactive,\n",
    "    create_self_consistency_quiz,\n",
    "    run_self_consistency_full_dataset,\n",
    "    # Comparison\n",
    "    display_method_comparison,\n",
    "    # Progress Tracker\n",
    "    run_progress_tracker_and_validation\n",
    ")\n",
    "\n",
    "# Load dataset\n",
    "DATASET = pathlib.Path('data/sentiment_tiny.csv')\n",
    "rows = []\n",
    "with open(DATASET, newline='', encoding='utf-8') as f:\n",
    "    r = csv.DictReader(f)\n",
    "    for row in r:\n",
    "        rows.append((row['text'], row['label']))\n",
    "\n",
    "# Load progress\n",
    "progress = load_progress()\n",
    "quiz = progress.setdefault('quiz', {})\n",
    "\n",
    "for old_key in ['intro', 'Testar notebook', 'zero_shot', 'few_shot']:\n",
    "    quiz.pop(old_key, None)\n",
    "\n",
    "# Display configuration\n",
    "print('='*60)\n",
    "print('âœ… Setup complete!')\n",
    "print('='*60)\n",
    "print(f'ğŸ“Š Dataset loaded: {len(rows)} examples')\n",
    "print(f'ğŸ¤– LLM Backend: {MODEL_BACKEND}')\n",
    "if MODEL_BACKEND == 'ollama':\n",
    "    print(f'   Model: {OLLAMA_MODEL}')\n",
    "elif MODEL_BACKEND == 'openai':\n",
    "    print(f'   Model: {OPENAI_MODEL}')\n",
    "print(f'ğŸ“ Progress tracking ready')\n",
    "print('='*60)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da27cd4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 1 â€” Zero-shot\n",
    "\n",
    "<div style=\"background:#dbeafe; border-left:6px solid #2563eb; padding:16px 20px; border-radius:6px; color:#1e3a8a; line-height:1.6;\">\n",
    "<strong style=\"font-size:16px;\">Zero-shot prompting</strong> means giving the model only an instruction or question, <strong>with no examples</strong>. The model must rely solely on its <strong>pretrained knowledge</strong> and <strong>understanding of instructions</strong>. It works best for clear, factual tasks and is cost-efficient since it requires fewer tokens.\n",
    "\n",
    "<strong style=\"margin-top:12px; display:block;\">Zero-shot prompting is powerful when:</strong>\n",
    "<ul style=\"margin:8px 0 12px 20px;\">\n",
    "  <li>You have <strong>no labeled data</strong>.</li>\n",
    "  <li>The task is <strong>clearly described</strong>.</li>\n",
    "  <li>You want to quickly test a model's reasoning or world knowledge.</li>\n",
    "</ul>\n",
    "\n",
    "<em style=\"color:#1e40af;\">However, it can sometimes produce inconsistent results since the model has no examples to guide its reasoning.</em>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64abcae",
   "metadata": {},
   "source": [
    "### 1.1 Example: Zero-shot Classification\n",
    "\n",
    "<div style=\"background:#eff6ff; border-left:4px solid #3b82f6; padding:16px; margin:16px 0; border-radius:6px; color:#1e3a8a;\">\n",
    "<strong>ğŸ“š What happens in this cell?</strong><br><br>\n",
    "You will see a <strong>complete step-by-step example</strong> of how zero-shot prompting works.\n",
    "\n",
    "<strong>ğŸ“ Example sentence used:</strong><br>\n",
    "<em>\"The movie was absolutely wonderful and full of emotion.\"</em>\n",
    "\n",
    "<strong>What you will see:</strong>\n",
    "- ğŸ“„ The input sentence and expected label\n",
    "- ğŸ” The exact prompt sent to the model\n",
    "- ğŸ¤– The model's raw response + normalized result\n",
    "- ğŸ“Š A summary table with the classification result\n",
    "- â±ï¸ Response latency (how long it took)\n",
    "\n",
    "<strong>ğŸ’¡ Pay attention to:</strong> The prompt is <em>very simple</em> â€“ just instructions, no examples!\n",
    "</div>\n",
    "\n",
    "Run the cell below to see the demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd6d4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Example: Zero-shot Classification ===\n",
    "display_zero_shot_example(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a57bd5",
   "metadata": {},
   "source": [
    "### 1.2 Try it yourself!\n",
    "\n",
    "<div style=\"background:#eff6ff; border-left:4px solid #3b82f6; padding:16px; margin:16px 0; border-radius:6px; color:#1e3a8a;\">\n",
    "<strong>ğŸ¯ Now it's your turn!</strong><br><br>\n",
    "Test zero-shot prompting with <strong>your own sentence</strong>.\n",
    "\n",
    "<strong>What will happen:</strong>\n",
    "- You enter a sentence (e.g., \"This movie was terrible\")\n",
    "- The model classifies it as positive/negative\n",
    "- You see the entire process: prompt â†’ model â†’ result\n",
    "\n",
    "<strong>ğŸ’¡ Try experimenting with:</strong>\n",
    "- Clearly positive sentences: \"I loved it!\"\n",
    "- Clearly negative sentences: \"Awful and boring.\"\n",
    "- Difficult edge cases: \"It was okay, not great but not bad.\"\n",
    "\n",
    "<strong>â±ï¸ Expected time:</strong> ~2-3 seconds per classification\n",
    "</div>\n",
    "\n",
    "Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2431f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Try it yourself! (Zero-shot) ===\n",
    "create_zero_shot_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818d7b80",
   "metadata": {},
   "source": [
    "### 1.3 Quiz: Test Your Understanding\n",
    "\n",
    "<div style=\"background:#eff6ff; border-left:4px solid #3b82f6; padding:16px; margin:16px 0; border-radius:6px; color:#1e3a8a;\">\n",
    "<strong>ğŸ” Knowledge Check</strong><br><br>\n",
    "Before moving forward, make sure you understand the <strong>zero-shot concept</strong>.\n",
    "\n",
    "<strong>What you will get:</strong>\n",
    "- âœ… Multiple choice question with clickable radio buttons\n",
    "- ğŸ’¡ Immediate feedback (green = correct, red = incorrect)\n",
    "- ğŸ’¾ Automatic saving of correct answers to progress file\n",
    "\n",
    "<strong>ğŸ¯ Goal:</strong> Answer correctly to proceed to the next section!\n",
    "</div>\n",
    "\n",
    "Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4a5e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Quiz: Test Your Understanding ===\n",
    "create_zero_shot_quiz(progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483479ea",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>ğŸ’¡ Need hints? (click to expand)</summary> \n",
    "\n",
    "Think about what \"zero\" means in **zero-shot** - how many examples does the model see?\n",
    "\n",
    "To understand the concept better, ask your AI:\n",
    "> \"What are the advantages and drawbacks of zero-shot vs few-shot learning for text classification?\"\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93e7596",
   "metadata": {},
   "source": [
    "### 1.4 Run Zero-shot on Full Dataset\n",
    "\n",
    "<div style=\"background:#eff6ff; border-left:4px solid #3b82f6; padding:16px; margin:16px 0; border-radius:6px; color:#1e3a8a;\">\n",
    "<strong>ğŸš€ Time to run on the entire dataset!</strong><br><br>\n",
    "Now we will classify <strong>all 15 sentences</strong> in the dataset using zero-shot prompting.\n",
    "\n",
    "<strong>What will happen:</strong>\n",
    "1. â³ The model classifies each sentence (takes ~30-60 seconds total)\n",
    "2. ğŸ“Š You get a table with accuracy, precision, recall, F1-score\n",
    "3. ğŸ“ˆ A bar chart visualizes the results\n",
    "4. ğŸ¯ A target line shows if you reached 60% accuracy\n",
    "5. ğŸ“‹ Detailed breakdown (true positives, false positives, etc.)\n",
    "6. ğŸ’¾ All metrics are automatically saved to <code>progress/lesson_progress.json</code>\n",
    "\n",
    "<strong>ğŸ¯ Goal:</strong> Reach at least <strong>60% accuracy</strong> to pass this section!\n",
    "\n",
    "<strong>âš ï¸ Note:</strong> If zero-shot doesn't reach the target â€“ don't worry! That's why we have few-shot and CoT.\n",
    "</div>\n",
    "\n",
    "Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fe8e32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === ğŸš€ Run Zero-shot on Full Dataset ===\n",
    "metrics_z = run_zero_shot_full_dataset(rows, progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd780ca",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2 â€” Few-shot\n",
    "\n",
    "<div style=\"background:#f5f3ff; border-left:6px solid #8b5cf6; padding:16px 20px; border-radius:6px; color:#5b21b6; line-height:1.6;\">\n",
    "<strong style=\"font-size:16px;\">Few-shot prompting</strong> means providing the model with <strong>a few labeled examples</strong> before asking it to classify new text. These examples help the model understand the <strong>pattern</strong> and <strong>desired output format</strong>.\n",
    "\n",
    "<strong style=\"margin-top:12px; display:block;\">Few-shot prompting is effective when:</strong>\n",
    "<ul style=\"margin:8px 0 12px 20px;\">\n",
    "  <li>You have <strong>a small set of labeled examples</strong> (typically 2-10).</li>\n",
    "  <li>The task requires understanding <strong>specific patterns or style</strong>.</li>\n",
    "  <li>Zero-shot performance is insufficient and needs guidance.</li>\n",
    "</ul>\n",
    "\n",
    "<em style=\"color:#6d28d9;\">Few-shot prompting typically improves accuracy compared to zero-shot, but requires more tokens (and thus higher cost).</em>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbe69bd",
   "metadata": {},
   "source": [
    "### 2.1 Example: Few-shot Classification\n",
    "\n",
    "<div style=\"background:#f5f3ff; border-left:4px solid #8b5cf6; padding:16px; margin:16px 0; border-radius:6px; color:#5b21b6;\">\n",
    "<strong>ğŸ“š What happens in this cell?</strong><br><br>\n",
    "You will see how <strong>few-shot prompting</strong> works with demonstration examples.\n",
    "\n",
    "<strong>ğŸ“ Example sentence used:</strong><br>\n",
    "<em>\"The movie was absolutely wonderful and full of emotion.\"</em>\n",
    "\n",
    "<strong>What you will see:</strong>\n",
    "- ğŸ“„ The same example sentence as in zero-shot\n",
    "- ğŸ“š 3 demonstration examples shown to the model FIRST\n",
    "- ğŸ” The complete prompt (with all examples included)\n",
    "- ğŸ¤– The model's response after seeing the examples\n",
    "- ğŸ“Š Results in table format\n",
    "- â±ï¸ Response latency (how long it took)\n",
    "\n",
    "<strong>ğŸ’¡ Notice the difference:</strong> The prompt is now <em>much longer</em> because it contains examples!\n",
    "</div>\n",
    "\n",
    "Run the cell below to see the demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69af7f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Example: Few-shot Classification ===\n",
    "display_few_shot_example(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def00bc9",
   "metadata": {},
   "source": [
    "### 2.2 Try it yourself!\n",
    "\n",
    "<div style=\"background:#f5f3ff; border-left:4px solid #8b5cf6; padding:16px; margin:16px 0; border-radius:6px; color:#5b21b6;\">\n",
    "<strong>ğŸ¯ Test few-shot with your own sentence!</strong><br><br>\n",
    "\n",
    "<strong>What will happen:</strong>\n",
    "- You enter a sentence\n",
    "- The model sees 3 demonstration examples FIRST\n",
    "- Then it classifies your sentence\n",
    "- You see the full prompt (including all examples)\n",
    "\n",
    "<strong>ğŸ’¡ Experiment:</strong> Test the same sentences you used in zero-shot â€“ did the results improve now?\n",
    "</div>\n",
    "\n",
    "Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2257803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Try it yourself! (Few-shot) ===\n",
    "create_few_shot_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4adc9fa",
   "metadata": {},
   "source": [
    "### 2.3 Quiz: Test Your Understanding\n",
    "\n",
    "<div style=\"background:#f5f3ff; border-left:4px solid #8b5cf6; padding:16px; margin:16px 0; border-radius:6px; color:#5b21b6;\">\n",
    "<strong>ğŸ” Knowledge Check: Few-shot</strong><br><br>\n",
    "\n",
    "<strong>What you will get:</strong>\n",
    "- âœ… Multiple choice question about few-shot prompting\n",
    "- ğŸ’¡ Immediate feedback on your answer\n",
    "- ğŸ“ Explanation of why the answer is correct/incorrect\n",
    "- ğŸ’¾ Automatic saving upon correct answer\n",
    "\n",
    "<strong>ğŸ¯ Focus:</strong> Understand <em>the difference between zero-shot and few-shot</em>!\n",
    "</div>\n",
    "\n",
    "Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d71071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Quiz: Test Your Understanding ===\n",
    "create_few_shot_quiz(progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019179e8",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>ğŸ’¡ Need hints? (click to expand)</summary> \n",
    "\n",
    "What's the main difference between **Zero-shot** and **few-shot**? What does the model get to see in each approach?\n",
    "\n",
    "To understand the concept better, ask your AI:\n",
    "> \"What are the advantages of few-shot learning over zero-shot? When would the extra token cost be justified?\"\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c019d96",
   "metadata": {},
   "source": [
    "### 2.4 Run Few-shot on Full Dataset\n",
    "\n",
    "<div style=\"background:#f5f3ff; border-left:4px solid #8b5cf6; padding:16px; margin:16px 0; border-radius:6px; color:#5b21b6;\">\n",
    "<strong>ğŸš€ Run few-shot on all 15 sentences!</strong><br><br>\n",
    "\n",
    "<strong>What will happen:</strong>\n",
    "1. â³ Each sentence is classified with 3 demonstration examples (takes ~30-60 seconds)\n",
    "2. ğŸ“Š Metrics: accuracy, precision, recall, F1\n",
    "3. ğŸ“ˆ Bar chart for visualization\n",
    "4. ğŸ¯ Comparison against the 60% target\n",
    "5. ğŸ’¾ Automatically saved to progress\n",
    "\n",
    "<strong>ğŸ’¡ Compare the results:</strong> Did few-shot perform better than zero-shot? Why/why not?\n",
    "\n",
    "<strong>ğŸ’° Cost:</strong> Few-shot is more expensive than zero-shot (more tokens per prompt)!\n",
    "</div>\n",
    "\n",
    "Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290db1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Run Few-shot on Full Dataset ===\n",
    "metrics_fs = run_few_shot_full_dataset(rows, progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db383920",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3 â€“ Chain-of-Thought (CoT)\n",
    "\n",
    "<div style=\"background:#fce7f3; border-left:6px solid #ec4899; padding:16px 20px; border-radius:6px; color:#831843; line-height:1.6;\">\n",
    "<strong style=\"font-size:16px;\">Chain-of-Thought (CoT) prompting</strong> encourages the model to <strong>explain its reasoning step-by-step</strong> before giving a final answer. Instead of jumping directly to a conclusion, the model shows its thought process.\n",
    "\n",
    "<strong style=\"margin-top:12px; display:block;\">CoT prompting is powerful when:</strong>\n",
    "<ul style=\"margin:8px 0 12px 20px;\">\n",
    "  <li>The task requires <strong>complex reasoning</strong> or multi-step logic.</li>\n",
    "  <li>You want to <strong>understand how the model reached its conclusion</strong>.</li>\n",
    "  <li>Simple prompts give inconsistent or incorrect results.</li>\n",
    "</ul>\n",
    "\n",
    "<em style=\"color:#9d174d;\">By asking the model to \"show its work,\" we often get more accurate and interpretable results, especially for nuanced tasks like sentiment analysis.</em>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd58bbc",
   "metadata": {},
   "source": [
    "### 3.1 Example: Chain-of-Thought Classification\n",
    "\n",
    "<div style=\"background:#fdf4ff; border-left:4px solid #ec4899; padding:16px; margin:16px 0; border-radius:6px; color:#831843;\">\n",
    "<strong>ğŸ“š What happens in this cell?</strong><br><br>\n",
    "Now you will see how the model <strong>\"thinks out loud\"</strong> before answering.\n",
    "\n",
    "<strong>ğŸ“ Example sentence used:</strong><br>\n",
    "<em>\"The movie was absolutely wonderful and full of emotion.\"</em>\n",
    "\n",
    "<strong>What you will see:</strong>\n",
    "- ğŸ“„ The example sentence (same as before)\n",
    "- ğŸ” A prompt asking the model to explain its reasoning\n",
    "- ğŸ§  The model's <strong>complete reasoning</strong> (step-by-step)\n",
    "- ğŸ¯ The extracted final answer (positive/negative)\n",
    "- ğŸ“Š Results table\n",
    "\n",
    "<strong>ğŸ’¡ Notice:</strong> The output is now <em>much longer</em> because the model explains its thought process!\n",
    "</div>\n",
    "\n",
    "Run the cell below to see the demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2482ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Example: Chain-of-Thought Classification ===\n",
    "display_cot_example(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd64a475",
   "metadata": {},
   "source": [
    "### 3.2 Try it yourself!\n",
    "\n",
    "<div style=\"background:#fdf4ff; border-left:4px solid #ec4899; padding:16px; margin:16px 0; border-radius:6px; color:#831843;\">\n",
    "<strong>ğŸ¯ Test Chain-of-Thought with your own sentence!</strong><br><br>\n",
    "\n",
    "<strong>What will happen:</strong>\n",
    "- You enter a sentence\n",
    "- The model gets instructions to \"show its reasoning step-by-step\"\n",
    "- You see the model's <strong>thought process</strong> (reasoning)\n",
    "- The last word in the reasoning becomes the answer\n",
    "\n",
    "<strong>ğŸ’¡ Use this for:</strong>\n",
    "- Difficult edge cases: \"It was okay but not great\"\n",
    "- Complex sentences: \"The acting was good but the plot was terrible\"\n",
    "- See <em>why</em> the model chose its answer!\n",
    "</div>\n",
    "\n",
    "Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e89e9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Try it yourself! (Chain-of-Thought) ===\n",
    "create_cot_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa62cd38",
   "metadata": {},
   "source": [
    "### 3.3 Quiz: Test Your Understanding\n",
    "\n",
    "<div style=\"background:#fdf4ff; border-left:4px solid #ec4899; padding:16px; margin:16px 0; border-radius:6px; color:#831843;\">\n",
    "<strong>ğŸ” Knowledge Check: Chain-of-Thought</strong><br><br>\n",
    "\n",
    "<strong>What you will get:</strong>\n",
    "- âœ… Question about CoT's main advantage\n",
    "- ğŸ’¡ Feedback and explanation\n",
    "- ğŸ’¾ Automatic progress saving\n",
    "\n",
    "<strong>ğŸ¯ Focus:</strong> Understand <em>why</em> explicit reasoning often leads to better results! Or not...\n",
    "</div>\n",
    "\n",
    "Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8c1439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Quiz: Test Your Understanding ===\n",
    "create_cot_quiz(progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e5f7fe",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>ğŸ’¡ Need hints? (click to expand)</summary> \n",
    "\n",
    "What does **Chain-of-Thought (CoT)** suggest about how the model should approach the problem?\n",
    "\n",
    "To understand the concept better, ask your AI:\n",
    "> \"Why does asking a model to show its reasoning often lead to better answers? What cognitive process does this mimic?\"\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dd7665",
   "metadata": {},
   "source": [
    "### 3.4 Run Chain-of-Thought on Full Dataset\n",
    "\n",
    "<div style=\"background:#fdf4ff; border-left:4px solid #ec4899; padding:16px; margin:16px 0; border-radius:6px; color:#831843;\">\n",
    "<strong>ğŸš€ Run CoT on all 15 sentences!</strong><br><br>\n",
    "\n",
    "<strong>What will happen:</strong>\n",
    "1. â³ Each sentence is classified WITH explicit reasoning (takes ~1-2 minutes)\n",
    "2. ğŸ“Š Metrics are calculated (accuracy, precision, recall, F1)\n",
    "3. ğŸ“ˆ Visualization of results\n",
    "4. ğŸ¯ Comparison against the 60% target\n",
    "5. ğŸ’¾ Saved to progress\n",
    "\n",
    "<strong>â±ï¸ Notice:</strong> CoT takes longer because the model generates more text!\n",
    "\n",
    "<strong>ğŸ’¡ Compare:</strong> Did CoT perform better than zero-shot and few-shot? Which method is best so far?\n",
    "</div>\n",
    "\n",
    "Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0600b648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Run Chain-of-Thought on Full Dataset ===\n",
    "metrics_cot = run_cot_full_dataset(rows, progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4251ac1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 4 â€“ Self-Consistency\n",
    "\n",
    "<div style=\"background:#fef3c7; border-left:6px solid #f59e0b; padding:16px 20px; border-radius:6px; color:#78350f; line-height:1.6;\">\n",
    "<strong style=\"font-size:16px;\">Self-Consistency</strong> takes Chain-of-Thought prompting one step further. Instead of generating <strong>one reasoning path</strong>, it generates <strong>multiple diverse reasoning paths</strong> and then uses <strong>majority voting</strong> to select the most consistent answer.\n",
    "\n",
    "<strong style=\"margin-top:12px; display:block;\">Self-Consistency is powerful when:</strong>\n",
    "<ul style=\"margin:8px 0 12px 20px;\">\n",
    "  <li>You need <strong>high reliability</strong> and can afford extra compute cost.</li>\n",
    "  <li>A single reasoning path might be flawed or biased.</li>\n",
    "  <li>The task benefits from <strong>multiple perspectives</strong> before deciding.</li>\n",
    "</ul>\n",
    "\n",
    "<em style=\"color:#92400e;\">By sampling multiple reasoning chains with higher temperature and aggregating results, Self-Consistency often achieves the highest accuracy â€“ at the cost of kÃ— more API calls (typically k=5-10).</em>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af710ed",
   "metadata": {},
   "source": [
    "### 4.1 Example: Self-Consistency Classification\n",
    "\n",
    "<div style=\"background:#fffbeb; border-left:4px solid #f59e0b; padding:16px; margin:16px 0; border-radius:6px; color:#78350f;\">\n",
    "<strong>ğŸ“š What happens in this cell?</strong><br><br>\n",
    "Self-Consistency = CoT <strong>Ã— 5 times</strong> + voting!\n",
    "\n",
    "<strong>ğŸ“ Example sentence used:</strong><br>\n",
    "<em>\"The movie was absolutely wonderful and full of emotion.\"</em>\n",
    "\n",
    "<strong>What you will see:</strong>\n",
    "- ğŸ”„ The same sentence classified 5 TIMES (with temperature=0.7)\n",
    "- ğŸ§  5 different reasoning paths (the model thinks differently each time)\n",
    "- ğŸ—³ï¸ Majority voting (which answer got the most votes?)\n",
    "- ğŸ“Š Vote distribution (e.g., \"positive: 4/5, negative: 1/5\")\n",
    "- ğŸ¯ Final answer based on majority\n",
    "\n",
    "<strong>âš ï¸ Warning:</strong> This is <em>5Ã— more expensive</em> than CoT (5 API calls per sentence)!\n",
    "\n",
    "<strong>ğŸ’¡ The point:</strong> By running multiple different thought processes and voting, we get more <em>reliable</em> results.\n",
    "</div>\n",
    "\n",
    "Run the cell below to see the demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891d7dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Example: Self-Consistency Classification ===\n",
    "display_self_consistency_example(rows, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eee701a",
   "metadata": {},
   "source": [
    "### 4.2 Try it yourself!\n",
    "\n",
    "<div style=\"background:#fffbeb; border-left:4px solid #f59e0b; padding:16px; margin:16px 0; border-radius:6px; color:#78350f;\">\n",
    "<strong>ğŸ¯ Test Self-Consistency with your own sentence!</strong><br><br>\n",
    "\n",
    "<strong>What will happen:</strong>\n",
    "- You enter a sentence\n",
    "- The model runs <strong>5 reasoning paths</strong> (takes ~10-15 seconds)\n",
    "- You see all 5 thought processes (may be different!)\n",
    "- Majority vote determines the final answer\n",
    "- You see vote distribution (e.g., 4/5 positive)\n",
    "\n",
    "<strong>ğŸ’¡ Test with:</strong>\n",
    "- Difficult edge cases where you're unsure\n",
    "- Compare: was the consensus answer more reliable?\n",
    "\n",
    "<strong>ğŸ’° Cost:</strong> 5Ã— more API calls = much more expensive!\n",
    "</div>\n",
    "\n",
    "Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a29aefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Try it yourself! (Self-Consistency) ===\n",
    "create_self_consistency_interactive(k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8ec278",
   "metadata": {},
   "source": [
    "### 4.3 Quiz: Test Your Understanding\n",
    "\n",
    "<div style=\"background:#fffbeb; border-left:4px solid #f59e0b; padding:16px; margin:16px 0; border-radius:6px; color:#78350f;\">\n",
    "<strong>ğŸ” Knowledge Check: Self-Consistency</strong><br><br>\n",
    "\n",
    "<strong>What you will get:</strong>\n",
    "- âœ… Question about the Self-Consistency mechanism\n",
    "- ğŸ’¡ Feedback and explanation\n",
    "- ğŸ’¾ Progress saving\n",
    "\n",
    "<strong>ğŸ¯ Focus:</strong> Understand the <em>trade-off</em> between cost and reliability!\n",
    "</div>\n",
    "\n",
    "Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b58ae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Quiz: Test Your Understanding ===\n",
    "create_self_consistency_quiz(progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26821307",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>ğŸ’¡ Need hints? (click to expand)</summary> \n",
    "\n",
    "Think about what happens when you ask multiple experts to solve a problem independently, then going with the consensus.\n",
    "\n",
    "To understand the concept better, ask your AI:\n",
    "> \"Why does sampling multiple reasoning paths with higher temperature and then voting often lead to better results than a single deterministic path?\"\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eb1978",
   "metadata": {},
   "source": [
    "### 4.4 Run Self-Consistency on Full Dataset\n",
    "\n",
    "<div style=\"background:#fffbeb; border-left:4px solid #f59e0b; padding:16px; margin:16px 0; border-radius:6px; color:#78350f;\">\n",
    "<strong>ğŸš€ Run Self-Consistency on all 15 sentences!</strong><br><br>\n",
    "\n",
    "<strong>What will happen:</strong>\n",
    "1. â³ Each sentence is classified 5 TIMES (takes ~4-6 minutes total!)\n",
    "2. ğŸ“Š Metrics are calculated from majority-voted results\n",
    "3. ğŸ“ˆ Visualization and comparison\n",
    "4. ğŸ¯ Check against the 60% target\n",
    "5. ğŸ’¾ Saved to progress\n",
    "\n",
    "<strong>ğŸ’° Total cost:</strong> 15 sentences Ã— 5 paths = <strong>75 API calls</strong>\n",
    "\n",
    "<strong>ğŸ’¡ Expected:</strong> Self-Consistency should give the <em>highest accuracy</em> of all methods (but at the highest cost)!\n",
    "\n",
    "<strong>ğŸ‰ After this cell:</strong> You have run all 4 methods and can compare them in the next section!\n",
    "</div>\n",
    "\n",
    "Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f56e704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Run Self-Consistency on Full Dataset ===\n",
    "metrics_sc = run_self_consistency_full_dataset(rows, progress, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51103481",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Comparison & Measurement\n",
    "\n",
    "<div style=\"background:#f3f4f6; border-left:6px solid #6b7280; padding:20px; margin:20px 0; border-radius:6px; color:#1f2937; line-height:1.6;\">\n",
    "<strong style=\"font-size:17px;\">ğŸ“Š Time to Compare All Methods!</strong><br><br>\n",
    "\n",
    "You've now run <strong>all four prompting methods</strong> on the same dataset. In this section, you will:\n",
    "<ul style=\"margin:12px 0 12px 20px; line-height:1.8;\">\n",
    "  <li><strong>Compare performance:</strong> Which method achieved the best accuracy?</li>\n",
    "  <li><strong>Analyze trade-offs:</strong> Cost vs. accuracy â€” when is each method worth it?</li>\n",
    "  <li><strong>Understand metrics:</strong> What do precision, recall, and F1-score actually mean?</li>\n",
    "  <li><strong>Reflect critically:</strong> Answer reflection questions about your findings</li>\n",
    "</ul>\n",
    "\n",
    "<strong style=\"display:block; margin-top:12px;\">ğŸ¯ Your Task:</strong>\n",
    "<ol style=\"margin:8px 0 0 20px; line-height:1.8;\">\n",
    "  <li>Review the comparison tables and charts below</li>\n",
    "  <li>Identify which method performed best (and why)</li>\n",
    "  <li>Answer the <strong>3 reflection questions</strong> at the end</li>\n",
    "  <li>Click <strong>\"ğŸ’¾ Save Reflections\"</strong> to complete the lesson</li>\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### Understanding the Metrics\n",
    "\n",
    "<div style=\"background:#fef3c7; border-left:4px solid #f59e0b; padding:16px; margin:16px 0; border-radius:6px; color:#78350f;\">\n",
    "<strong>ğŸ“š What do these metrics mean?</strong><br><br>\n",
    "\n",
    "<strong>1. Accuracy</strong> â€” Overall correctness<br>\n",
    "<em style=\"color:#92400e;\">Formula: (Correct predictions) / (Total predictions)</em><br>\n",
    "Example: If the model correctly classifies 12 out of 15 sentences â†’ Accuracy = 80%<br>\n",
    "<strong>When to use:</strong> Good for balanced datasets, but can be misleading if classes are imbalanced.<br><br>\n",
    "\n",
    "<strong>2. Precision</strong> â€” How accurate are the positive predictions?<br>\n",
    "<em style=\"color:#92400e;\">Formula: (True Positives) / (True Positives + False Positives)</em><br>\n",
    "Example: Of all sentences predicted as \"positive\", how many were actually positive?<br>\n",
    "<strong>When to use:</strong> Important when false positives are costly (e.g., spam detection).<br><br>\n",
    "\n",
    "<strong>3. Recall</strong> â€” How many actual positives did we find?<br>\n",
    "<em style=\"color:#92400e;\">Formula: (True Positives) / (True Positives + False Negatives)</em><br>\n",
    "Example: Of all actually positive sentences, how many did we correctly identify?<br>\n",
    "<strong>When to use:</strong> Important when missing positives is costly (e.g., disease detection).<br><br>\n",
    "\n",
    "<strong>4. F1-Score</strong> â€” Balanced measure of precision and recall<br>\n",
    "<em style=\"color:#92400e;\">Formula: 2 Ã— (Precision Ã— Recall) / (Precision + Recall)</em><br>\n",
    "Harmonic mean that balances both precision and recall.<br>\n",
    "<strong>When to use:</strong> Best overall metric when you care about both false positives and false negatives.<br><br>\n",
    "\n",
    "<strong>ğŸ’¡ Key Insight:</strong> High accuracy doesn't always mean a good model! Always look at precision, recall, and F1 together.\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### Confusion Matrix Terminology\n",
    "\n",
    "<div style=\"background:#dbeafe; border-left:4px solid #3b82f6; padding:16px; margin:16px 0; border-radius:6px; color:#1e3a8a;\">\n",
    "<strong>ğŸ” Understanding True/False Positives/Negatives</strong><br><br>\n",
    "\n",
    "<table style=\"width:100%; border-collapse: collapse; margin-top:12px;\">\n",
    "  <tr style=\"background:#eff6ff;\">\n",
    "    <th style=\"padding:12px; border:1px solid #bfdbfe; text-align:center;\"></th>\n",
    "    <th style=\"padding:12px; border:1px solid #bfdbfe; text-align:center;\">Predicted: Positive</th>\n",
    "    <th style=\"padding:12px; border:1px solid #bfdbfe; text-align:center;\">Predicted: Negative</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding:12px; border:1px solid #bfdbfe; font-weight:bold;\">Actually: Positive</td>\n",
    "    <td style=\"padding:12px; border:1px solid #bfdbfe; background:#d1fae5; text-align:center;\">\n",
    "      <strong>True Positive (TP)</strong><br>\n",
    "      âœ… Correctly identified positive\n",
    "    </td>\n",
    "    <td style=\"padding:12px; border:1px solid #bfdbfe; background:#fee2e2; text-align:center;\">\n",
    "      <strong>False Negative (FN)</strong><br>\n",
    "      âŒ Missed a positive (predicted negative)\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding:12px; border:1px solid #bfdbfe; font-weight:bold;\">Actually: Negative</td>\n",
    "    <td style=\"padding:12px; border:1px solid #bfdbfe; background:#fee2e2; text-align:center;\">\n",
    "      <strong>False Positive (FP)</strong><br>\n",
    "      âŒ Wrong positive alert (predicted positive)\n",
    "    </td>\n",
    "    <td style=\"padding:12px; border:1px solid #bfdbfe; background:#d1fae5; text-align:center;\">\n",
    "      <strong>True Negative (TN)</strong><br>\n",
    "      âœ… Correctly identified negative\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<br>\n",
    "<strong>ğŸ’¡ Example:</strong><br>\n",
    "If a model predicts \"positive\" for 8 sentences:<br>\n",
    "- 6 were actually positive â†’ <strong>6 True Positives (TP)</strong><br>\n",
    "- 2 were actually negative â†’ <strong>2 False Positives (FP)</strong><br>\n",
    "- Precision = 6/(6+2) = 75%\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "Run the cell below to see the complete comparison and answer the reflection questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc34bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Method Comparison: Overview & Reflection ===\n",
    "display_method_comparison(progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a69ae13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Main Takeaways\n",
    "\n",
    "### ğŸ¯ What You Learned\n",
    "\n",
    "### **1. Zero-shot Prompting**\n",
    "- âœ… How zero-shot prompts work (instruction only, no examples)\n",
    "- âœ… Best for simple, clearly-defined tasks\n",
    "- âœ… Lowest token cost (most efficient)\n",
    "- âœ… Quick to implement and test\n",
    "\n",
    "### **2. Few-shot Prompting**\n",
    "- âœ… How to provide demonstrations in prompts\n",
    "- âœ… Demonstrations are included **before** the task input\n",
    "- âœ… More tokens = higher cost than zero-shot\n",
    "- âœ… Helps model learn patterns from examples\n",
    "\n",
    "### **3. Chain-of-Thought (CoT)**\n",
    "- âœ… Asking models to show reasoning step-by-step\n",
    "- âœ… Improves performance on complex tasks\n",
    "- âœ… Longer responses (more tokens)\n",
    "- âœ… Reasoning transparency helps debugging\n",
    "\n",
    "### **4. Self-Consistency**\n",
    "- âœ… Running multiple CoT samples and voting\n",
    "- âœ… Most reliable but most expensive (kÃ—cost)\n",
    "- âœ… Best for high-stakes decisions\n",
    "- âœ… Aggregates diverse reasoning paths\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Trade-offs Summary\n",
    "\n",
    "| Method | Cost | Accuracy | Use When |\n",
    "|--------|------|----------|----------|\n",
    "| **Zero-shot** | ğŸ’° | Variable | Simple tasks, budget constraints |\n",
    "| **Few-shot** | ğŸ’°ğŸ’° | Better | Need pattern examples |\n",
    "| **CoT** | ğŸ’°ğŸ’° | High (complex) | Complex reasoning needed |\n",
    "| **Self-Consistency** | ğŸ’°ğŸ’°ğŸ’° | Highest | Critical decisions |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¡ Understanding Your Results\n",
    "\n",
    "<div style=\"background:#e0f2fe; border-left:4px solid #0284c7; padding:16px; margin:16px 0; border-radius:6px; color:#075985;\">\n",
    "<strong>ğŸ“Š A note about the trade-offs table above:</strong><br><br>\n",
    "\n",
    "The table shows **general expectations**, but your actual results might differ!\n",
    "\n",
    "You may notice that:\n",
    "- âœ… **Zero-shot performed surprisingly well** â€” Sometimes simplicity wins\n",
    "- âš ï¸ **Few-shot wasn't always better** â€” Bad examples can hurt more than help\n",
    "- ğŸ¤” **CoT didn't always improve accuracy** â€” Requires careful prompt design\n",
    "- ğŸ¯ **Self-Consistency was most reliable** â€” Voting reduces variance\n",
    "\n",
    "<strong>Why do results vary from expectations?</strong>\n",
    "- ğŸ“ **Dataset difficulty** â€” With clear sentiment words, all methods work well\n",
    "- ğŸ¤– **Model capability** â€” Strong models (like llama3.2-long) can handle complexity in zero-shot\n",
    "- ğŸ² **Prompt quality** â€” Generic CoT prompts may not always trigger better reasoning\n",
    "- ğŸ“Š **Sample size** â€” With only 15 examples, small variations matter\n",
    "\n",
    "<strong>ğŸ’¼ Key lesson:</strong> Never trust theoretical trade-offs blindly! Always benchmark on YOUR data with YOUR model before choosing a method in production.\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’­ Further Reflection \n",
    "\n",
    "Take a moment to consider what you've learned:\n",
    "\n",
    "1. **Which method surprised you the most in terms of performance?** Did the results match your expectations?\n",
    "\n",
    "2. **When would you choose few-shot over CoT?** Think about real-world scenarios where token cost matters versus reasoning transparency.\n",
    "\n",
    "3. **How did temperature affect Self-Consistency?** What happens if temperature is too low (0.0) or too high (1.5)?\n",
    "\n",
    "4. **What limitations did you observe?** Consider edge cases, truncation issues, or when prompting alone might not be enough.\n",
    "\n",
    "5. **How would you apply this in production?** Balance accuracy, cost, latency, and reliability for a real application.\n",
    "\n",
    "---\n",
    "### ğŸš€ Next Steps\n",
    "\n",
    "Now that you understand the core prompting techniques, consider exploring:\n",
    "\n",
    "- **RAG (Retrieval-Augmented Generation)** â€“ Combine prompting with knowledge retrieval\n",
    "- **Fine-tuning (LoRA/QLoRA)** â€“ Adapt models to specific domains\n",
    "- **Auto-prompting** â€“ Automatically optimize prompts\n",
    "- **Experiment tracking** â€“ MLOps practices (LangFuse, Weights & Biases)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** ğŸ‰ You've completed the Prompt Engineering Lab!\n",
    "\n",
    "Run `python src/verify.py` to validate your work and generate your completion receipt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f50fee9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Progress tracker\n",
    "\n",
    "Run the autotest and see a compact progress report. If the autograder prints PASS and a receipt is created, you are done.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba021fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Progress Tracker & Validation ===\n",
    "run_progress_tracker_and_validation(progress, project_root)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
